#!/usr/bin/env python3
# -*- coding: utf-8 -*-

''' TorSpider â€“ A script to explore the darkweb.
    -------------------by Christopher Steffen---

    TorSpider will explore the darkweb to discover as many onion sites as
    possible, storing them all in a database along with whatever additional
    information can be found. It will also store data regarding which sites
    connected to which other sites, allowing for some relational mapping.

    The database generated by TorSpider will be accessible via a secondary
    script which will create a web interface for exploring the saved data.
'''

import os
import sys
import time
import random
import requests
import sqlite3 as sql
from hashlib import sha1
from datetime import datetime
from html.parser import HTMLParser
from urllib.parse import urlsplit, urlunsplit
from multiprocessing import Process, cpu_count, current_process


'''---GLOBAL VARIABLES---'''


# Should we log to the console?
log_to_console = True

# Let's use the default Tor Browser Bundle UA:
agent = 'Mozilla/5.0 (Windows NT 6.1; rv:52.0) Gecko/20100101 Firefox/52.0'

# Just to prevent some SSL errors.
requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += \
                                              ':ECDHE-ECDSA-AES128-GCM-SHA256'


'''---FUNCTION DEFINITIONS---'''


def crawl():



            '''---[ CONTINUE FROM HERE ]---'''
            # Get the page's links.
            page_links = get_links(page_text, url)

            # Add the links to the database.
            for link_url in page_links:
                # Get the link domain.
                link_url = fix_url(link_url)
                link_domain = get_domain(link_url)
                try:
                    # Insert the new domain into the onions table.
                    db_cmd("INSERT OR IGNORE INTO onions (domain) \
                           VALUES (?);", [link_domain])
                    # Insert the new link into the pages table.
                    db_cmd("INSERT OR IGNORE INTO pages (domain, url) \
                           VALUES ((SELECT id FROM onions WHERE \
                           domain = ?), ?);", [link_domain, link_url])
                    # Insert the new connection between domains.
                    db_cmd("INSERT OR IGNORE INTO links (domain, link) \
                           VALUES (?, (SELECT id FROM onions WHERE \
                           domain = ?));", [domain_id, link_domain])
                except Exception as e:
                    # There was an error saving the link to the database.
                    log('Failed to save to database: {} -> {}'.format(url, l))
                    continue
            # Parsing is complete for this page!




def defrag_domain(domain):
    # Defragment the given domain.
    domain_parts = domain.split('.')
    # Onion domains don't have strange symbols or numbers in them, so be
    # sure to remove any of those just in case someone's obfuscating domains
    # for some reason.
    domain_parts[-2] = ''.join(ch for ch in domain_parts[-2] if ch.isalnum())
    domain = '.'.join(domain_parts)
    return domain


def fix_url(url):
    # Fix obfuscated urls.
    (scheme, netloc, path, query, fragment) = urlsplit(url)
    netloc = defrag_domain(netloc)
    url = urlunsplit((scheme, netloc, path, query, fragment))
    return url


def get_domain(url):
    # Get the defragmented domain of the given url.
    domain = defrag_domain(urlsplit(url)[1])
    # Let's omit subdomains. Rather than having separate records for urls like
    # sub1.onionpage.onion and sub2.onionpage.onion, just keep them all under
    # onionpage.onion.
    domain = '.'.join(domain.split('.')[-2:])
    return domain





def get_links(data, url):
    # Given HTML input, return a list of all unique links.
    parse = ParseLinks()
    parse.feed(data)
    links = []
    domain = urlsplit(url)[1]
    for link in parse.output_list:
        if(link is None):
            # Skip empty links.
            continue
        # Remove any references to the current directory. ('./')
        link = link.replace('./', '')
        # Split the link into its component parts.
        (scheme, netloc, path, query, fragment) = urlsplit(link)
        # Fill in empty schemes.
        scheme = 'http' if scheme is '' else scheme
        # Fill in empty paths.
        path = '/' if path is '' else path
        if(netloc is '' and '.onion' in path.split('/')[0]):
            # The urlparser mistook the domain as part of the path.
            netloc = path.split('/')[0]
            try:
                path = '/'.join(path.split('/')[1:])
            except Exception as e:
                path = '/'
        # Fill in empty domains.
        netloc = domain if netloc is '' else netloc
        fragment = ''
        if('.onion' not in netloc or '.onion.' in netloc):
            # We are only interested in links to other .onion domains.
            continue
        links.append(urlunsplit((scheme, netloc, path, query, fragment)))
    # Make sure we don't return any duplicates!
    return unique(links)
