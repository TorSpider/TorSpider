#!/usr/bin/env python
# -*- coding: utf-8 -*-

''' TorSpider â€“ A script to explore the darkweb.
    -------------------by Christopher Steffen---
    
    Usage: TorSpider.py [Seed URL]
    
        If no Seed URL is provided, TorSpider will begin scanning wherever
        it left off last time, then will re-scan all known URLs from the top
        of the list.
    
    --------------------------------------------
    
    TorSpider will explore the darkweb to discover as many onion sites as
    possible, storing them all in a database along with whatever additional
    information can be found. It will also store data regarding which sites
    connected to which other sites, allowing for some relational mapping.
    
    The database generated by TorSpider will be accessible via a secondary
    script which will create a web interface for exploring the saved data.
'''

'''---INCLUDES---'''

import requests, sys, sqlite3 as sql
from HTMLParser import HTMLParser

'''---VARIABLES---'''

# How many threads do we want the spider to run on this system?
max_threads = 5     # Five threads shouldn't be too heavy a load, even for a Pi.
recursion_depth = 3 # How many links deep should we delve into any particular URL?

'''---CLASSES---'''

class MyParser(HTMLParser):
    # Parse given HTML for all a.href and img.src links.
    def __init__(self, output_list=None):
        HTMLParser.__init__(self)
        if output_list is None:
            self.output_list = []
        else:
            self.output_list = output_list
    def handle_starttag(self, tag, attrs):
        if tag == 'a':
            self.output_list.append(dict(attrs).get('href'))
        elif tag == 'img':
            self.output_list.append(dict(attrs).get('src'))

'''---FUNCTIONS---'''

def get_links(data):
    # Given HTML input, return a list of all unique http or ftp links.
    p = MyParser()
    p.feed(data)
    links = []
    for link in p.output_list:
        try:
            if(('http' in link) and link not in links):
                links.append(link)
        except:
            pass
    return links

def get_domain(link):
    # Given a link, extract the domain.
    s = link.split('/')
    for i in s:
        if(i != 'http:' and i != 'https:' and i != ''):
            return i

def get_unique_domains(data):
    # Given HTML input, return a list of all unique domains.
    domains = []
    for link in get_links(data):
        domain = get_domain(link)
        if(domain not in domains):
            domains.append(domain)
    return domains

def get_tor_session():
    # Create a session that's routed through Tor.
    session = requests.session()
    session.proxies = {'http': 'socks5://127.0.0.1:9050', 'https':'socks5://127.0.0.1:9050'}
    return session

def db_cmd(cmd):
    # This function executes commands in the database.
    rtn = None
    con = sql.connect('SpiderWeb.db')
    cur = con.cursor()
    try:
        buf = cmd.strip()
        cur.execute(buf)
        if(buf.upper().startswith("SELECT")):
            rtn = cur.fetchall()
    except sql.Error as e:
        print("SQL ERROR -- %s" % (e))
    con.commit()
    con.close()
    if(rtn != None):
        try:
            (rtn, ) = rtn[0]
        except:
            rtn = None
    return rtn

def crawl(url):
    ''' This is the primary spider function. Given a URL, it'll collect information on the
        page and crawl along all links, adding external URLs to one database and crawling
        up to recursion_depth levels deep scanning for new URLs within this TLD.
    '''
    pass # Keep working!

'''---PREPARATION---'''

# Create a new Tor session.
session = get_tor_session()

# Just to prevent some SSL errors.
requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':ECDHE-ECDSA-AES128-GCM-SHA256'

# Spoof a specific user agent (tor browser).
session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; rv:52.0) Gecko/20100101 Firefox/52.0'})

# Determine if the user has provided a Seed URL or asked for usage information.
seed_url = 'http://zqktlwi4fecvo6ri.onion/wiki/Main_Page' # This is the Uncensored Hidden Wiki URL
try:
    seed_url = sys.argv[1]
except:
    pass
if(seed_url == '--help' or seed_url == '-h'):
    print '''
Usage: TorSpider.py [Seed URL]

    If no Seed URL is provided, TorSpider will begin scanning wherever
    it left off last time, then will re-scan all known URLs from the top
    of the list.
'''
    sys.exit(0)

# First, let's see if we're able to connect through Tor.
try:
    local_ip = requests.get('http://icanhazip.com').text
    tor_ip = session.get('http://icanhazip.com').text
    
    print "Local IP: %s" % (local_ip.strip())
    print "Tor IP:   %s" % (tor_ip.strip())
    if(local_ip != tor_ip):
        print "Tor connection successful!"
    else:
        print "Tor connection unsuccessful."
        sys.exit(0)
except:
    print "Tor connection unsuccessful."
    sys.exit(0)

'''---SQL INITIALIZATION---'''

# The following two databases are constant. They store the current state of the spider and a list of all TLDs we've discovered so far.
# Additional tables will be created for each specific onion domain in which pages and links to other TLDs will be stored.

# The 'onions' database stores a list of all TLDs, including their id, domain, last online status, and the number of times they've been seen offline.
db_cmd('CREATE TABLE IF NOT EXISTS `onions` (`id` INTEGER PRIMARY KEY,`domain` TEXT, `online` INTEGER, `offline_count` INTEGER);')

# The `state` database keeps certain information about the last run, so we can pick up on reboot.
db_cmd('CREATE TABLE IF NOT EXISTS `state` (`last_id` INTEGER);')

'''---MAIN---'''

# Let's demonstrate by grabbing unique domains from the seed URL and printing them out.
data = session.get(seed_url).text
domains = get_unique_domains(data)
for domain in domains:
    print domain